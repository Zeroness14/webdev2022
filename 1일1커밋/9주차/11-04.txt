인공신경망(ANN : Artificial Neural Network) - 2

* 퍼셉트론

 - 계단 함수 또는 부호함수를 사용하여 만들어진 단순한 뉴런

 - 퍼셉트론에서는 초평면과 선형분리 개념이 적용된다.

  > 초평면 : N차원 공간을 두개의 영역으로 나누는 평면

  > 선형분리 : 값의 분포를 2개로 나눠지는 평면이 존재하면, 선형분리가 가능하다라고 한다.

 - 선형분리가 가능해야지 퍼셉트론으로 표현이 가능하다.

 ex1) AND

   > 녹색선과 빨간색선은 모두 2차원 공간을 2개로 나누는 초평면이다.

   > 녹색선으로 만들어진 함수는 AND를 나타내기 위해 부적합하다.

   > 붉은선으로 만들어진 함수는 AND를 나타내기에 적합하다. 또한 붉은선에 의해서 상태가 나눠지므로,

      선형분리가 가능하다.

   > 이는 퍼셉트론으로 계산이 가능하다.
 

 ex2) XOR

  

   > 어떠한 초평면으로도 XOR을 만족시키는 것이 없다.

   > 이는 선형분리가 불가능하며, 퍼셉트론으로 계산이 불가능하다.



* 퍼셉트론의 학습규칙

 - 퍼셉트론의 학습은 오차율을 계산해내어, 입력값의 비율 만큼 가중치 값을 조종한다.

 - 다음과 같은 AND연산을 위한 퍼셉트론에서 (x1, x2)가 (1, 1)이 들어 왔을 때,

   가중치 값을 변경해 나가는 것을 보며 학습규칙을 설명하겠다.

 

  1) 현재 들어 온 값과 Step함수를 이용해 Y값을 구한다.

     f(X1*W1 + X2+W2 - θ ) = f(1*0.2 + 1*(-0.1) - 0.2 ) = f( - 0.1) = 0 (Step 함수는 0보다 클 경우 1을 작을경우 -1을 출력)

  2) (1, 1)이 들어 왔을 때에 기대 값은 1이므로, 오차 "1"이 발생한다.

   > e = Yd - Y ( Yd : 목표치 / Y : 현재치 )

  3) 오차가 발생했기 때문에 보정을 해준다. 이 보정은 다음의 식을 따른다.

   > Wi(next) = Wi(current) + a * Xi * e // 다음 가중치 = 현재 가중치 + 보정치 * 입력값 * 오차 // a = 0.1 

     ( a는 0과 1사이의 값으로 학습률을 나타낸다. 이 값은 Wi값이 적정치로 가는데 값의 변화의 폭을 줄이기 위해서 필요하다. )

   > 위 식에 따라 W1(next) = 0.2 + 0.1 * 1 * 1 = 0.3, W2(next) = -0.1 + 0.1 * 1 * 1 = 0.0으로 계산된다.

 - 위와 같은 형식으로 가중치 값을 학습해나간다. 이는 오차를 누가 더 많이 발생했느냐에 따라서 가중치 값의 변화도 변한다.

 - 위 과정은 가능한 모든 입력치에 대해서 모두 실행했을때 모두 오차가 나지 않을 때 까지 반복한다.

  > 하나의 입력치 뭉치를 에폭이라고 하며, 가중치 값을 한번 변화하는 과정을 P값을 이용해 나타낸다.

 - 위에서 알아본 퍼셉트론의 학습규칙을 일반화하면 다음과 같다.



 ⓐ Step 1 : 초기 가중치 값 및 임계값(θ)를 임의의 값으로 부여한다. 여기서 θ와 W값은 -0.5~0.5 사이를 만족시키도록 한다.  

                 또한 P값을 1로 Setting한다. 또한, 정한 학습률을 저장시킨다.(슈도코드에선 0.1로 선택)

     >> Pseudo Code 

     theta = Random[-0.5,0.5];                                    // Set the theta value( threshold )

     for ( i = 0 ; i < n ; i++ ) W[i] = Random[-0.5,0.5];    //  Set the W[i] value( weight )

     p = 1;                                                                //  Set the P value ( Proceed number )

     assistant = 0.1;                                                   //  Set the a value



 ⓑ Step 2 : 활성화 함수와 theta값을 이용하여 Y(p)값을 산출해난다.

                 

    >> Pseudo Code

    sum = 0;                                                             // sum  value of Xi * Wi

    result;                                                                // Value of Y(i)

    // Calculate Step Function

    for ( i = 0 ; i < n ; i++ ) sum += x[i]*w[i];                // sum of Xi * Wi

    sum -= theta;                                                      // Minus threshold value

    if ( sum > 0 ) result = 1;                                        

    else result = 0;



 ⓒ Step 3 : 다음 식을 이용하여 가중치 보정을 실시한다. 에러가 발생하지 않았다면 Step 4로 간다.

               



    >> Pseudo Code

    error = expectation[p] - result;

    if ( error != 0 ){

        for ( i = 0 ; i < n ; i++ ) w[i] += assistant * x[i] * e;

    }

 ⓓ Step 4 : 한 에폭을 실시하는 동안 에러가 나지 않는 다면 종료, 하나라도 에러가 났다면 Step 2로 돌아간다.



* 퍼셉트론의 한계

 - 선형분리가 불가능한 문제에 대해서 해결 할 수 없었다.(XOR)

 - 이러한 문제를 풀기 위해서 나온 것이 다층 피드 포워드 신경망이다.